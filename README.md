# letta-graph-chat: Letta-style Memory Graph Agent Demo

A modern memory-augmented conversational agent, built with LangGraph and inspired by Lettaâ€™s memory system.  
Unlike standard chat memory, this agent can **summarize, store, and reuse knowledge across sessions** using a hybrid short/long-term memory architecture.

## Why Letta-style Memory?

Typical conversational AI agents using LangChain's built-in memory only retain short-term chat history (windowed buffer or simple summary).  
This PoC implements a Letta-inspired hybrid memory structure, combining:

- **Short-term memory:** Recent chat turns for context continuity
- **Long-term memory:** Periodic session summaries, stored in a vector database for scalable recall and re-injection
- **Knowledge aggregation:** Summaries and context can be reused across sessions, not just within a single chat

**Why does this matter?**  

- Enables agents to "remember" across sessions (persistent context)
- Avoids context bloat and token overflow by summarizing and pruning
- Supports scalable, extensible graph-based workflows (add more memory types, tools, or logic nodes as needed)

## Comparison with LangChain's Standard Memory

|                    | LangChain Standard Memory | Letta-Graph Memory (This PoC) |
|--------------------|--------------------------|-------------------------------|
| Short-term History | âœ…                        | âœ…                            |
| Long-term Summary  | âŒ                        | âœ… (vector DB, persistent)    |
| Knowledge Transfer | âŒ (session-limited)      | âœ… (session-spanning)         |
| Extensibility      | â–³ (limited)              | âœ… (graph/node-based)         |

## ðŸ”§ Features

- Letta-style hybrid memory (short/long-term, summary vector store)
- Session-spanning knowledge aggregation and recall
- Graph-based memory flow for extensibility (easy to add tools, logic, memory layers)

## ðŸŽ¯ Use Cases

- Knowledge worker assistants
- FAQ/chatbots with persistent context
- Personal context managers

## ðŸ“‚ Structure

```bash
memory-demo/
â”œâ”€â”€ main.py              # Streamlit app  
â”œâ”€â”€ langgraph_demo.py    # Graph construction and agent logic  
â”œâ”€â”€ memory_adapter.py    # ChromaMemoryAdapter implementation  
â”œâ”€â”€ .env.example         # Sample API key file  
â””â”€â”€ pyproject.toml       # uv-based env
```

## ðŸš€ How to Run

```bash
uv sync
uv run streamlit run main.py
```

Set your `OPENAI_API_KEY` in `.env`.

## Sample Input & Output

```text
User: What are some good points about Tokyo?
Tool: Tokyo is safe, has great public transport, and offers amazing food.

User: What is the weather like in Tokyo today?
Tool: Tokyo's weather today is mostly sunny with a high of 27Â°C.

User: Can you recommend some popular sightseeing spots in Tokyo?
Tool: Famous tourist spots in Tokyo include Asakusa, Tokyo Tower, and Shibuya Crossing.
```

## ðŸ§  Memory Flow

1. Input is checked for summarization intent  
2. If summarizing: all history is collected â†’ summary generated â†’ stored in Chroma  
3. If answering: memory is retrieved â†’ prompt is built with history â†’ response saved  

## ðŸ” Tech Stack

- Python 3.12  
- LangChain / LangGraph  
- Streamlit  
- Chroma (for vector memory)

## ðŸ“¹ Optional

A demo GIF showing the interaction flow is available.
TODO:

## Architecture

the graph

```mermaid
flowchart TD
    A[load_summary] --> B[load_memory]
    B --> C[decide]
    C -->|answer| D[run_agent]
    C -->|summarize| E[summarize_session]
    D --> F[save_memory]
    E --> G[END]
    F --> G
```

## Disclaimer:

This project is provided as a proof of concept (PoC) for demonstration purposes only.
Use at your own risk. No warranty or support is provided for production use.
